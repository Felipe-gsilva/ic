Existem 2 MLP's atuando paralelamente G e D, geradora e discriminadora, as quais entram sempre em um equilibrio ao final do treinamento. Elas vivem em constante aflição e agonia na competitividade extrema do capitalismo.

em especifico, a função $V(G, D)$ é dada por:

$$
\min_{G} \max_{D} V(D,G) = \mathbb{E}_{\boldsymbol{x} \sim p_{\mathrm{data}}(\boldsymbol{x})[\log D(\boldsymbol{x})] + \mathbb{E}_{\boldsymbol{z} \sim p_{\boldsymbol{z}}}(\boldsymbol{z})[\log(1 - D(G(\boldsymbol{z})))].
$$


- $D(x)$ representa a probabilidade de que x faz parte do domínio (não foi gerado).
- $G(z)$ representa a saída de uma MLP dado um input de noise $z$.

de maneira geral, essa função indica que a rede G, a qual gera $x$, tende a minimizar a capacidade do discriminador detectar suas amostras. Isso é dado pelo segundo componente.


Como a saida é normalizada, sabemos que D(X) é a chance de uma imagem x fazer parte do espaço de dados (dataset, data space ou dominio), enquanto 1 - D(x) é dado como a prob de não o ser (ser, portanto, gerada por G).

`Equilíbrio de Nash`: A solução ideal ocorre quando GG gera dados com distribuição idêntica à real ($p_{data} = p_G$), e D não consegue distinguir (ou seja, $D(x)=0.5$ para qualquer x).

Um problema possível dessa rede é o fato de que, durante o começo do treinamento, a rede D consegue facilmente distinguir entre uma imagem real ou não, portanto, G não seria muito bem treinada, ou seja, esta equação não provê um gradiente suficiente para o treino de G. O componente $log(1 - D(G(z)))$ satura, sem treinar G corretamente.
